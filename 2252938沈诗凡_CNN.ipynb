{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "## 2252938æ²ˆè¯—å‡¡_CNN",
   "id": "e43ed38b13092172"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MNIST",
   "id": "773f3b44be1ca92f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:37:47.463219Z",
     "start_time": "2025-11-13T10:37:42.866077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ],
   "id": "311a6fa1a02d369a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:37:47.554986Z",
     "start_time": "2025-11-13T10:37:47.477235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== æ•°æ®å‡†å¤‡ ==========\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# ä¸‹è½½MNISTæ•°æ®é›†\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ9:1ï¼‰\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# æ•°æ®åŠ è½½å™¨\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ],
   "id": "cbf275fb37615597",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:37:47.567726Z",
     "start_time": "2025-11-13T10:37:47.560846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== å®šä¹‰æ¨¡å‹ ==========\n",
    "# å…¨è¿æ¥ç½‘ç»œï¼ˆMLPï¼‰\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          # 14x14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)           # 7x7\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # å±•å¹³\n",
    "        return self.fc_layers(x)"
   ],
   "id": "cc285a6e905de73",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:37:47.596058Z",
     "start_time": "2025-11-13T10:37:47.591008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== è®­ç»ƒä¸éªŒè¯å‡½æ•° ==========\n",
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "    acc = 100. * correct / len(train_loader.dataset)\n",
    "    return total_loss / len(train_loader), acc\n",
    "\n",
    "def evaluate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    acc = 100. * correct / len(val_loader.dataset)\n",
    "    return total_loss / len(val_loader), acc"
   ],
   "id": "f16d98c70dd376dc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:37:47.627588Z",
     "start_time": "2025-11-13T10:37:47.620589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== ä¸»è®­ç»ƒæµç¨‹ ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_experiment(model, name, epochs=5):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    print(f\"\\nå¼€å§‹è®­ç»ƒæ¨¡å‹ï¼š{name}\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss, train_acc = train(model, device, train_loader, optimizer, criterion)\n",
    "        val_loss, val_acc = evaluate(model, device, val_loader, criterion)\n",
    "        print(f\"Epoch {epoch}: è®­ç»ƒé›† Acc={train_acc:.2f}%  éªŒè¯é›† Acc={val_acc:.2f}%\")\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "    print(f\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{test_acc:.2f}%\")\n",
    "    return test_acc\n"
   ],
   "id": "ed304e742302289e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:40:26.463696Z",
     "start_time": "2025-11-13T10:37:47.651654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========== æ¨¡å‹å¯¹æ¯” ==========\n",
    "mlp_acc = run_experiment(MLP(), \"å…¨è¿æ¥ç½‘ç»œï¼ˆMLPï¼‰\")\n",
    "cnn_acc = run_experiment(CNN(), \"å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰\")\n",
    "\n",
    "print(\"\\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœï¼š\")\n",
    "print(f\"MLP æµ‹è¯•é›†å‡†ç¡®ç‡: {mlp_acc:.2f}%\")\n",
    "print(f\"CNN æµ‹è¯•é›†å‡†ç¡®ç‡: {cnn_acc:.2f}%\")\n"
   ],
   "id": "41268974ce586ad8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼šå…¨è¿æ¥ç½‘ç»œï¼ˆMLPï¼‰\n",
      "Epoch 1: è®­ç»ƒé›† Acc=89.12%  éªŒè¯é›† Acc=93.57%\n",
      "Epoch 2: è®­ç»ƒé›† Acc=94.88%  éªŒè¯é›† Acc=95.60%\n",
      "Epoch 3: è®­ç»ƒé›† Acc=96.26%  éªŒè¯é›† Acc=95.87%\n",
      "Epoch 4: è®­ç»ƒé›† Acc=96.99%  éªŒè¯é›† Acc=96.70%\n",
      "Epoch 5: è®­ç»ƒé›† Acc=97.48%  éªŒè¯é›† Acc=96.90%\n",
      "æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š97.26%\n",
      "\n",
      "å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰\n",
      "Epoch 1: è®­ç»ƒé›† Acc=94.82%  éªŒè¯é›† Acc=98.42%\n",
      "Epoch 2: è®­ç»ƒé›† Acc=98.50%  éªŒè¯é›† Acc=98.37%\n",
      "Epoch 3: è®­ç»ƒé›† Acc=98.98%  éªŒè¯é›† Acc=98.43%\n",
      "Epoch 4: è®­ç»ƒé›† Acc=99.26%  éªŒè¯é›† Acc=98.75%\n",
      "Epoch 5: è®­ç»ƒé›† Acc=99.42%  éªŒè¯é›† Acc=99.02%\n",
      "æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š99.10%\n",
      "\n",
      "ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœï¼š\n",
      "MLP æµ‹è¯•é›†å‡†ç¡®ç‡: 97.26%\n",
      "CNN æµ‹è¯•é›†å‡†ç¡®ç‡: 99.10%\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CIFAR-10",
   "id": "c19a9cb865beeee7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:40:26.577944Z",
     "start_time": "2025-11-13T10:40:26.574992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F"
   ],
   "id": "62132eaa0a970bb0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:40:28.008345Z",
     "start_time": "2025-11-13T10:40:26.599745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========= æ•°æ®å‡†å¤‡ ==========\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# CIFAR-10 æ•°æ®é›†ï¼ˆ50000è®­ç»ƒ + 10000æµ‹è¯•ï¼‰\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†ä¸éªŒè¯é›†ï¼ˆ9:1ï¼‰\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ],
   "id": "53e081ba532f64ef",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:40:28.033143Z",
     "start_time": "2025-11-13T10:40:28.021140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========= æ¨¡å‹å®šä¹‰ ==========\n",
    "\n",
    "# åŸºç¡€CNN\n",
    "class BaseCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),   # 16x16\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)    # 8x8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# æ¨¡å‹2ï¼šåœ¨åˆ†ç±»å™¨å‰å†å¢åŠ ä¸€ä¸ªå·ç§¯å—\n",
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*8*8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# æ¨¡å‹3ï¼šæ¯ä¸ªæ± åŒ–å‰å†åŠ ä¸€å±‚å·ç§¯ï¼ˆæ›´å®½ï¼‰\n",
    "class WideCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WideCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),  # å¢åŠ \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),  # å¢åŠ \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# æ¨¡å‹ 4: ResNet\n",
    "# å®šä¹‰ResNetæ¨¡å‹\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = torch.nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ],
   "id": "23f5e7a11629162",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:40:28.052438Z",
     "start_time": "2025-11-13T10:40:28.048078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========= è®­ç»ƒä¸éªŒè¯å‡½æ•° ==========\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), acc\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), acc\n"
   ],
   "id": "3daf700533521b28",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T10:40:28.079600Z",
     "start_time": "2025-11-13T10:40:28.074446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========= è¿è¡Œå®éªŒ ==========\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def run_model(model, name, epochs=10):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(f\"\\nè®­ç»ƒæ¨¡å‹: {name}\")\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch:02d}: è®­ç»ƒAcc={train_acc*100:.2f}%  éªŒè¯Acc={val_acc*100:.2f}%\")\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    print(f\"{name} æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc*100:.2f}%\")\n",
    "    return test_acc\n"
   ],
   "id": "277e05fb79bea42d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T11:14:56.074170Z",
     "start_time": "2025-11-13T10:40:28.097986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========= æ¯”è¾ƒä¸åŒæ¨¡å‹ ==========\n",
    "models = {\n",
    "    \"BaseCNN\": BaseCNN(),\n",
    "    \"DeepCNN\": DeepCNN(),\n",
    "    \"WideCNN\": WideCNN(),\n",
    "    \"ResNet\": ResNet(BasicBlock, [2, 2, 2])\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    acc = run_model(model, name, epochs=10)\n",
    "    results[name] = acc\n"
   ],
   "id": "450c69f9c88f6829",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "è®­ç»ƒæ¨¡å‹: BaseCNN\n",
      "Epoch 01: è®­ç»ƒAcc=49.30%  éªŒè¯Acc=59.16%\n",
      "Epoch 02: è®­ç»ƒAcc=63.83%  éªŒè¯Acc=67.06%\n",
      "Epoch 03: è®­ç»ƒAcc=69.64%  éªŒè¯Acc=68.58%\n",
      "Epoch 04: è®­ç»ƒAcc=74.32%  éªŒè¯Acc=70.40%\n",
      "Epoch 05: è®­ç»ƒAcc=77.99%  éªŒè¯Acc=71.80%\n",
      "Epoch 06: è®­ç»ƒAcc=81.35%  éªŒè¯Acc=72.50%\n",
      "Epoch 07: è®­ç»ƒAcc=85.15%  éªŒè¯Acc=71.70%\n",
      "Epoch 08: è®­ç»ƒAcc=88.58%  éªŒè¯Acc=72.40%\n",
      "Epoch 09: è®­ç»ƒAcc=91.56%  éªŒè¯Acc=73.30%\n",
      "Epoch 10: è®­ç»ƒAcc=93.89%  éªŒè¯Acc=72.08%\n",
      "BaseCNN æµ‹è¯•é›†å‡†ç¡®ç‡: 71.16%\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: DeepCNN\n",
      "Epoch 01: è®­ç»ƒAcc=50.26%  éªŒè¯Acc=63.56%\n",
      "Epoch 02: è®­ç»ƒAcc=66.44%  éªŒè¯Acc=67.72%\n",
      "Epoch 03: è®­ç»ƒAcc=73.45%  éªŒè¯Acc=73.18%\n",
      "Epoch 04: è®­ç»ƒAcc=78.54%  éªŒè¯Acc=73.54%\n",
      "Epoch 05: è®­ç»ƒAcc=83.25%  éªŒè¯Acc=73.64%\n",
      "Epoch 06: è®­ç»ƒAcc=87.76%  éªŒè¯Acc=75.54%\n",
      "Epoch 07: è®­ç»ƒAcc=91.96%  éªŒè¯Acc=75.40%\n",
      "Epoch 08: è®­ç»ƒAcc=94.82%  éªŒè¯Acc=75.68%\n",
      "Epoch 09: è®­ç»ƒAcc=96.56%  éªŒè¯Acc=74.60%\n",
      "Epoch 10: è®­ç»ƒAcc=97.61%  éªŒè¯Acc=75.74%\n",
      "DeepCNN æµ‹è¯•é›†å‡†ç¡®ç‡: 75.60%\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: WideCNN\n",
      "Epoch 01: è®­ç»ƒAcc=45.97%  éªŒè¯Acc=56.82%\n",
      "Epoch 02: è®­ç»ƒAcc=64.02%  éªŒè¯Acc=67.80%\n",
      "Epoch 03: è®­ç»ƒAcc=71.45%  éªŒè¯Acc=71.74%\n",
      "Epoch 04: è®­ç»ƒAcc=76.86%  éªŒè¯Acc=73.56%\n",
      "Epoch 05: è®­ç»ƒAcc=81.44%  éªŒè¯Acc=74.32%\n",
      "Epoch 06: è®­ç»ƒAcc=85.61%  éªŒè¯Acc=75.22%\n",
      "Epoch 07: è®­ç»ƒAcc=89.80%  éªŒè¯Acc=74.74%\n",
      "Epoch 08: è®­ç»ƒAcc=92.98%  éªŒè¯Acc=73.50%\n",
      "Epoch 09: è®­ç»ƒAcc=95.54%  éªŒè¯Acc=73.66%\n",
      "Epoch 10: è®­ç»ƒAcc=96.22%  éªŒè¯Acc=74.64%\n",
      "WideCNN æµ‹è¯•é›†å‡†ç¡®ç‡: 73.33%\n",
      "\n",
      "è®­ç»ƒæ¨¡å‹: ResNet\n",
      "Epoch 01: è®­ç»ƒAcc=50.14%  éªŒè¯Acc=56.80%\n",
      "Epoch 02: è®­ç»ƒAcc=65.25%  éªŒè¯Acc=57.62%\n",
      "Epoch 03: è®­ç»ƒAcc=71.48%  éªŒè¯Acc=70.60%\n",
      "Epoch 04: è®­ç»ƒAcc=75.63%  éªŒè¯Acc=74.56%\n",
      "Epoch 05: è®­ç»ƒAcc=78.51%  éªŒè¯Acc=73.08%\n",
      "Epoch 06: è®­ç»ƒAcc=80.43%  éªŒè¯Acc=76.00%\n",
      "Epoch 07: è®­ç»ƒAcc=82.80%  éªŒè¯Acc=76.14%\n",
      "Epoch 08: è®­ç»ƒAcc=84.28%  éªŒè¯Acc=75.04%\n",
      "Epoch 09: è®­ç»ƒAcc=85.78%  éªŒè¯Acc=78.98%\n",
      "Epoch 10: è®­ç»ƒAcc=87.13%  éªŒè¯Acc=75.74%\n",
      "ResNet æµ‹è¯•é›†å‡†ç¡®ç‡: 75.45%\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T11:14:56.189426Z",
     "start_time": "2025-11-13T11:14:56.181094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========= è¾“å‡ºæœ€ä¼˜æ¨¡å‹ ==========\n",
    "best_model = max(results, key=results.get)\n",
    "print(\"\\næ€§èƒ½å¯¹æ¯”ç»“æœï¼š\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v*100:.2f}%\")\n",
    "\n",
    "print(f\"\\næœ€ä¼˜æ¨¡å‹: {best_model} ï¼Œå‡†ç¡®ç‡ï¼š{results[best_model]*100:.2f}%\")"
   ],
   "id": "919eb066f316b785",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ€§èƒ½å¯¹æ¯”ç»“æœï¼š\n",
      "BaseCNN: 71.16%\n",
      "DeepCNN: 75.60%\n",
      "WideCNN: 73.33%\n",
      "ResNet: 75.45%\n",
      "\n",
      "æœ€ä¼˜æ¨¡å‹: DeepCNN ï¼Œå‡†ç¡®ç‡ï¼š75.60%\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
