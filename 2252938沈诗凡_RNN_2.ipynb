{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T08:31:38.002497Z",
     "start_time": "2025-11-29T08:31:35.060232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "任务二：用 PyTorch 实现 LSTM / Stacked LSTM / BiLSTM（使用 Keras 的 IMDB 数据集）\n",
    "保存模型并打印评估报告（accuracy, precision, recall, f1）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n"
   ],
   "id": "f81116d9bcdc5e4e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T08:31:38.024044Z",
     "start_time": "2025-11-29T08:31:38.005973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# 超参数\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "vocab_size = 20000\n",
    "maxlen = 200\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_layers_stacked = 2  # 堆叠 LSTM 的层数\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "lr = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_dir = \"saved_models_task2\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# 固定随机种子\n",
    "# -----------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()\n"
   ],
   "id": "33b56d02d29ec807",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T08:31:40.274061Z",
     "start_time": "2025-11-29T08:31:38.099676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# 加载并预处理数据（Keras IMDB）\n",
    "# -----------------------\n",
    "print(\"Loading IMDB dataset (keras)...\")\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# 转为 PyTorch Tensor\n",
    "x_train_t = torch.LongTensor(x_train)\n",
    "y_train_t = torch.LongTensor(y_train)\n",
    "x_test_t = torch.LongTensor(x_test)\n",
    "y_test_t = torch.LongTensor(y_test)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(IMDBDataset(x_train_t, y_train_t), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(IMDBDataset(x_test_t, y_test_t), batch_size=batch_size, shuffle=False)\n"
   ],
   "id": "cc2b6aacadf51377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset (keras)...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T08:31:46.676556Z",
     "start_time": "2025-11-29T08:31:46.670378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# -----------------------\n",
    "# 模型定义（forward 返回 logits，不做 sigmoid）\n",
    "# loss 中使用 BCEWithLogitsLoss\n",
    "# -----------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)                       # (B, T, E)\n",
    "        out, _ = self.lstm(emb)                       # out: (B, T, H)\n",
    "        last = out[:, -1, :]                          # 取最后时间步 (B, H)\n",
    "        logits = self.fc(last).squeeze(1)             # (B,)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class StackedLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.lstm(emb)\n",
    "        last = out[:, -1, :]\n",
    "        logits = self.fc(last).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.lstm(emb)                       # out: (B, T, 2H)\n",
    "        last = out[:, -1, :]                          # (B, 2H)\n",
    "        logits = self.fc(last).squeeze(1)             # (B,)\n",
    "        return logits\n"
   ],
   "id": "e6dff0af87d61e7f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T08:31:47.609498Z",
     "start_time": "2025-11-29T08:31:47.605743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# 训练与评估函数\n",
    "# -----------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for x_batch, y_batch in loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            logits = model(x_batch)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            preds = (probs >= 0.5).astype(int)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(y_batch.numpy())\n",
    "\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_labels)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"report\": report, \"cm\": cm}\n"
   ],
   "id": "72e91c51f6fbdd35",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T08:31:48.653274Z",
     "start_time": "2025-11-29T08:31:48.648870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----------------------\n",
    "# 训练并保存模型的通用流程\n",
    "# -----------------------\n",
    "def run_experiment(model_class, model_name, model_kwargs, epochs=epochs, save=True):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Experiment: {model_name} | params: {model_kwargs}\")\n",
    "    model = model_class(**model_kwargs).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        metrics = evaluate_model(model, test_loader)\n",
    "        print(f\"[{model_name}] Epoch {epoch}/{epochs}  TrainLoss: {train_loss:.4f}  TestAcc: {metrics['accuracy']:.4f}  F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "        # 保存最佳模型（按 F1）\n",
    "        if metrics[\"f1\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1\"]\n",
    "            best_state = {\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "                \"epoch\": epoch,\n",
    "                \"metrics\": metrics\n",
    "            }\n",
    "\n",
    "    print(f\"\\nBest test F1 for {model_name}: {best_f1:.4f}\")\n",
    "    print(\"Classification report (best epoch):\\n\")\n",
    "    print(best_state[\"metrics\"][\"report\"])\n",
    "    print(\"Confusion matrix:\\n\", best_state[\"metrics\"][\"cm\"])\n",
    "\n",
    "    if save and best_state is not None:\n",
    "        save_path = os.path.join(save_dir, f\"{model_name}_best.pth\")\n",
    "        torch.save(best_state, save_path)\n",
    "        print(f\"Saved best model to: {save_path}\")\n",
    "\n",
    "    return best_state\n"
   ],
   "id": "ef052b258772c776",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T08:32:30.009351Z",
     "start_time": "2025-11-29T08:31:50.176020Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# main: 依次运行三种模型\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 单层 LSTM\n",
    "    lstm_kwargs = {\"vocab_size\": vocab_size, \"embed_dim\": embed_dim, \"hidden_dim\": hidden_dim}\n",
    "    best_lstm = run_experiment(LSTMClassifier, \"LSTM_single\", lstm_kwargs)\n",
    "\n",
    "    # 堆叠 LSTM（num_layers_stacked）\n",
    "    stacked_kwargs = {\"vocab_size\": vocab_size, \"embed_dim\": embed_dim, \"hidden_dim\": hidden_dim, \"num_layers\": num_layers_stacked}\n",
    "    best_stacked = run_experiment(StackedLSTMClassifier, \"LSTM_stacked\", stacked_kwargs)\n",
    "\n",
    "    # BiLSTM\n",
    "    bilstm_kwargs = {\"vocab_size\": vocab_size, \"embed_dim\": embed_dim, \"hidden_dim\": hidden_dim}\n",
    "    best_bilstm = run_experiment(BiLSTMClassifier, \"BiLSTM\", bilstm_kwargs)\n",
    "\n",
    "    print(\"\\nAll experiments finished.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Experiment: LSTM_single | params: {'vocab_size': 20000, 'embed_dim': 128, 'hidden_dim': 128}\n",
      "[LSTM_single] Epoch 1/5  TrainLoss: 0.5814  TestAcc: 0.7706  F1: 0.7528\n",
      "[LSTM_single] Epoch 2/5  TrainLoss: 0.5071  TestAcc: 0.7108  F1: 0.6815\n",
      "[LSTM_single] Epoch 3/5  TrainLoss: 0.3993  TestAcc: 0.8242  F1: 0.8189\n",
      "[LSTM_single] Epoch 4/5  TrainLoss: 0.4311  TestAcc: 0.7552  F1: 0.7388\n",
      "[LSTM_single] Epoch 5/5  TrainLoss: 0.3760  TestAcc: 0.8250  F1: 0.8119\n",
      "\n",
      "Best test F1 for LSTM_single: 0.8189\n",
      "Classification report (best epoch):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8064    0.8533    0.8292     12500\n",
      "           1     0.8442    0.7951    0.8189     12500\n",
      "\n",
      "    accuracy                         0.8242     25000\n",
      "   macro avg     0.8253    0.8242    0.8241     25000\n",
      "weighted avg     0.8253    0.8242    0.8241     25000\n",
      "\n",
      "Confusion matrix:\n",
      " [[10666  1834]\n",
      " [ 2561  9939]]\n",
      "Saved best model to: saved_models_task2\\LSTM_single_best.pth\n",
      "\n",
      "============================================================\n",
      "Experiment: LSTM_stacked | params: {'vocab_size': 20000, 'embed_dim': 128, 'hidden_dim': 128, 'num_layers': 2}\n",
      "[LSTM_stacked] Epoch 1/5  TrainLoss: 0.5871  TestAcc: 0.7299  F1: 0.7091\n",
      "[LSTM_stacked] Epoch 2/5  TrainLoss: 0.5095  TestAcc: 0.8102  F1: 0.8115\n",
      "[LSTM_stacked] Epoch 3/5  TrainLoss: 0.3518  TestAcc: 0.8571  F1: 0.8559\n",
      "[LSTM_stacked] Epoch 4/5  TrainLoss: 0.2516  TestAcc: 0.8642  F1: 0.8664\n",
      "[LSTM_stacked] Epoch 5/5  TrainLoss: 0.1945  TestAcc: 0.8702  F1: 0.8745\n",
      "\n",
      "Best test F1 for LSTM_stacked: 0.8745\n",
      "Classification report (best epoch):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8977    0.8355    0.8655     12500\n",
      "           1     0.8462    0.9048    0.8745     12500\n",
      "\n",
      "    accuracy                         0.8702     25000\n",
      "   macro avg     0.8719    0.8702    0.8700     25000\n",
      "weighted avg     0.8719    0.8702    0.8700     25000\n",
      "\n",
      "Confusion matrix:\n",
      " [[10444  2056]\n",
      " [ 1190 11310]]\n",
      "Saved best model to: saved_models_task2\\LSTM_stacked_best.pth\n",
      "\n",
      "============================================================\n",
      "Experiment: BiLSTM | params: {'vocab_size': 20000, 'embed_dim': 128, 'hidden_dim': 128}\n",
      "[BiLSTM] Epoch 1/5  TrainLoss: 0.5878  TestAcc: 0.7543  F1: 0.7555\n",
      "[BiLSTM] Epoch 2/5  TrainLoss: 0.4399  TestAcc: 0.8070  F1: 0.8161\n",
      "[BiLSTM] Epoch 3/5  TrainLoss: 0.3712  TestAcc: 0.8337  F1: 0.8294\n",
      "[BiLSTM] Epoch 4/5  TrainLoss: 0.2793  TestAcc: 0.8370  F1: 0.8254\n",
      "[BiLSTM] Epoch 5/5  TrainLoss: 0.2314  TestAcc: 0.8450  F1: 0.8430\n",
      "\n",
      "Best test F1 for BiLSTM: 0.8430\n",
      "Classification report (best epoch):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8365    0.8576    0.8469     12500\n",
      "           1     0.8539    0.8323    0.8430     12500\n",
      "\n",
      "    accuracy                         0.8450     25000\n",
      "   macro avg     0.8452    0.8450    0.8449     25000\n",
      "weighted avg     0.8452    0.8450    0.8449     25000\n",
      "\n",
      "Confusion matrix:\n",
      " [[10720  1780]\n",
      " [ 2096 10404]]\n",
      "Saved best model to: saved_models_task2\\BiLSTM_best.pth\n",
      "\n",
      "All experiments finished.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
